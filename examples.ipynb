{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08dea17f-bb5a-460d-82e5-f57600597af8",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4777f90c-ea16-48ed-bf40-e1277e447e58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rar.retrievers import BM25Retriever, DenseRetriever, AggregationType\n",
    "from rar.encoders import HuggingFaceEncoder\n",
    "from rar.utils import load_corpus, load_eval_dataset, evaluate_retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197a1501-a320-482e-9257-a213fdb9dce8",
   "metadata": {},
   "source": [
    "# Evaluate BM25 on entity retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c56ee60-e590-48d9-877a-c175f8ca8abd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading full dbpedia_small dataset on entity_retrieval domain...\n"
     ]
    }
   ],
   "source": [
    "domain = 'entity_retrieval'\n",
    "corpus = 'dbpedia_small'\n",
    "dataset = 'dbpedia_small'\n",
    "\n",
    "docs, referrals = load_corpus(domain, corpus)\n",
    "queries, ground_truth = load_eval_dataset(domain, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67cf2e90-b3ed-4928-94dd-179a0edc8577",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without referral augmentation...\n",
      "Recall@1: 0.6 +/- 0.058554004376911994\n",
      "Recall@10: 0.6428571428571429 +/- 0.05727026612409093\n",
      "\n",
      "With referral augmentation...\n",
      "Recall@1: 0.5857142857142857 +/- 0.058876755202770775\n",
      "Recall@10: 0.6714285714285714 +/- 0.056139144084518006\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Without referral augmentation...')\n",
    "retriever = BM25Retriever(docs)\n",
    "# multiple_correct refers to this dataset having multiple ground truths\n",
    "evaluate_retriever(retriever, queries, ground_truth, multiple_correct=True)\n",
    "print()\n",
    "\n",
    "print('With referral augmentation...')\n",
    "retriever = BM25Retriever(docs, referrals, aggregation=AggregationType.CONCAT)\n",
    "evaluate_retriever(retriever, queries, ground_truth, multiple_correct=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006468bc-4bf3-459a-b5d2-3e44d8ad5973",
   "metadata": {},
   "source": [
    "# Evaluate BM25 on paper retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71a96d04-8a29-4b64-aa35-c152b665ad60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading full acl_small dataset on paper_retrieval domain...\n"
     ]
    }
   ],
   "source": [
    "domain = 'paper_retrieval'\n",
    "corpus = 'acl_small'\n",
    "dataset = 'acl_small'\n",
    "\n",
    "docs, referrals = load_corpus(domain, corpus)\n",
    "queries, ground_truth = load_eval_dataset(domain, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d3ff6f6-47e6-43cb-935a-84b47f89cfdd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without referral augmentation...\n",
      "MRR: 0.20353690476190475 +/- 0.007746342940904251\n",
      "Recall@1: 0.1375 +/- 0.00770044641563072\n",
      "Recall@10: 0.3695 +/- 0.010792815897623752\n",
      "\n",
      "With referral augmentation...\n",
      "MRR: 0.32728710317460313 +/- 0.008669156027702959\n",
      "Recall@1: 0.216 +/- 0.009201738966086791\n",
      "Recall@10: 0.5855 +/- 0.011015664982196943\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Without referral augmentation...')\n",
    "retriever = BM25Retriever(docs)\n",
    "evaluate_retriever(retriever, queries, ground_truth)\n",
    "print()\n",
    "\n",
    "print('With referral augmentation...')\n",
    "retriever = BM25Retriever(docs, referrals, aggregation=AggregationType.CONCAT)\n",
    "evaluate_retriever(retriever, queries, ground_truth)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36301c1-5101-4026-a190-fae6db15e8d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluate Specter on paper retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49fae908-388b-4f21-bd8f-980b373b396f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d11cb0262c6542898c57416380044fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3fe6e4c4562442bb3c1945f7dd9a5f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ef5a0cf1e94b9590e3deaa6534c186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/321 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79c923de07e14e339d5a8a9e477bc871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/222k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0df2b0e9416473785ca2b996ee8c084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without referral augmentation...\n",
      "Encoding corpus...\n",
      "Not using referral augmentation\n",
      "Took 2148.4282710552216 seconds\n",
      "MRR: 0.13613373015873015 +/- 0.006454279448139866\n",
      "Recall@1: 0.0835 +/- 0.006185780063985462\n",
      "Recall@10: 0.28 +/- 0.010039920318408906\n",
      "\n",
      "With referral augmentation...\n",
      "MRR: 0.16907440476190477 +/- 0.007043126165034891\n",
      "Recall@1: 0.1055 +/- 0.006869124762296867\n",
      "Recall@10: 0.3405 +/- 0.010596219844831457\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder = HuggingFaceEncoder('allenai/specter')\n",
    "\n",
    "print('Without referral augmentation...')\n",
    "retriever = DenseRetriever(encoder, docs, verbose=True)\n",
    "evaluate_retriever(retriever, queries, ground_truth)\n",
    "print()\n",
    "\n",
    "print('With referral augmentation...')\n",
    "retriever = DenseRetriever(encoder, docs, referrals, aggregation=AggregationType.CONCAT)\n",
    "evaluate_retriever(retriever, queries, ground_truth)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39b6c26-3805-4e31-aa47-5670cee1fc2e",
   "metadata": {},
   "source": [
    "# Qualitative example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44663f5-7def-45a3-a869-226139ade356",
   "metadata": {},
   "source": [
    "One peculiar fact about the SWAG paper (Zellers et al 2018) is that it uses BERT's hyperparameters. What if we ask a retriever to recall this fact?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "423b351b-0538-4f94-b400-feb783e919ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[TITLE] A Latent Variable Model Approach to PMI-based Word Embeddings [ABSTRACT] Semantic word embeddings represent the meaning of a word via a vector, and are created by diverse methods. Many use nonlinear operations on co-occurrence statistics, and have hand-tuned hyperparameters and reweighting methods. This paper proposes a new generative model, a dynamic version of the log-linear topic model of Mnih and Hinton (2007) . The methodological novelty is to use the prior to compute closed form expressions for word statistics. This provides a theoretical justification for nonlinear models like PMI, word2vec, and GloVe, as well as some hyperparameter choices. It also helps explain why lowdimensional semantic embeddings contain linear algebraic structure that allows solution of word analogies, as shown by Mikolov et al. (2013a) and many subsequent papers. Experimental support is provided for the generative model assumptions, the most important of which is that latent word vectors are fairly uniformly dispersed in space.',\n",
       " \"[TITLE] Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems [ABSTRACT] Automatic machine learning systems can inadvertently accentuate and perpetuate inappropriate human biases. Past work on examining inappropriate biases has largely focused on just individual systems. Further, there is no benchmark dataset for examining inappropriate biases in systems. Here for the first time, we present the Equity Evaluation Corpus (EEC), which consists of 8,640 English sentences carefully chosen to tease out biases towards certain races and genders. We use the dataset to examine 219 automatic sentiment analysis systems that took part in a recent shared task, SemEval-2018 Task 1 'Affect in Tweets'. We find that several of the systems show statistically significant bias; that is, they consistently provide slightly higher sentiment intensity predictions for one race or one gender. We make the EEC freely available.\",\n",
       " '[TITLE] CoNLL-X Shared Task On Multilingual Dependency Parsing [ABSTRACT] Each year the Conference on Computational Natural Language Learning (CoNLL) 1 features a shared task, in which participants train and test their systems on exactly the same data sets, in order to better compare systems. The tenth CoNLL (CoNLL-X) saw a shared task on Multilingual Dependency Parsing. In this paper, we describe how treebanks for 13 languages were converted into the same dependency format and how parsing performance was measured. We also give an overview of the parsing approaches that participants took and the results that they achieved. Finally, we try to draw general conclusions about multi-lingual parsing: What makes a particular language, treebank or annotation scheme easier or harder to parse and which phenomena are challenging for any dependency parser? Many thanks to Amit Dubey and Yuval Krymolowski, the other two organizers of the shared task, for discussions, converting treebanks, writing software and helping with the papers. 2 1 see',\n",
       " '[TITLE] CAMR at SemEval-2016 Task 8: An Extended Transition-based AMR Parser [ABSTRACT] This paper describes CAMR, the transitionbased parser that we use in the SemEval-2016 Meaning Representation Parsing task. The main contribution of this paper is a description of the additional sources of information that we use as features in the parsing model to further boost its performance. We start with our existing AMR parser and experiment with three sets of new features: 1) rich named entities, 2) a verbalization list, 3) semantic role labels. We also use the RPI Wikifier to wikify the concepts in the AMR graph. Our parser achieves a Smatch F-score of 62% on the official blind test set.',\n",
       " '[TITLE] Multi-Event Extraction Guided by Global Constraints [ABSTRACT] This paper addresses the extraction of event records from documents that describe multiple events. Specifically, we aim to identify the fields of information contained in a document and aggregate together those fields that describe the same event. To exploit the inherent connections between field extraction and event identification, we propose to model them jointly. Our model is novel in that it integrates information from separate sequential models, using global potentials that encourage the extracted event records to have desired properties. While the model contains high-order potentials, efficient approximate inference can be performed with dualdecomposition. We experiment with two data sets that consist of newspaper articles describing multiple terrorism events, and show that our model substantially outperforms traditional pipeline models.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_no_rar = BM25Retriever(docs)\n",
    "retriever_no_rar.retrieve('paper that the SWAG paper took hyperparameters from', num_docs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d96a52-55f6-475a-8b49-b03997de007d",
   "metadata": {},
   "source": [
    "Without referrals, we get nonsense. With referrals, we get the most relevant papers as BERT and SWAG, as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "366519ba-9792-4e3a-96c6-36a80b0e1b4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[TITLE] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [ABSTRACT] We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018; Radford et al., 2018) , BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-theart models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4% (7.6% absolute improvement), MultiNLI accuracy to 86.7% (5.6% absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5 absolute improvement), outperforming human performance by 2.0.',\n",
       " '[TITLE] SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference [ABSTRACT] Given a partial description like \"she opened the hood of the car,\" humans can reason about the situation and anticipate what might come next (\"then, she examined the engine\"). In this paper, we introduce the task of grounded commonsense inference, unifying natural language inference and commonsense reasoning. We present Swag, a new dataset with 113k multiple choice questions about a rich spectrum of grounded situations. To address the recurring challenges of the annotation artifacts and human biases found in many existing datasets, we propose Adversarial Filtering (AF), a novel procedure that constructs a de-biased dataset by iteratively training an ensemble of stylistic classifiers, and using them to filter the data. To account for the aggressive adversarial filtering, we use state-of-theart language models to massively oversample a diverse set of potential counterfactuals. Empirical results demonstrate that while humans can solve the resulting inference problems with high accuracy (88%), various competitive models struggle on our task. We provide comprehensive analysis that indicates significant opportunities for future research.',\n",
       " \"[TITLE] Vector-space topic models for detecting Alzheimer's disease [ABSTRACT] Semantic deficit is a symptom of language impairment in Alzheimer's disease (AD). We present a generalizable method for automatic generation of information content units (ICUs) for a picture used in a standard clinical task, achieving high recall, 96.8%, of human-supplied ICUs. We use the automatically generated topic model to extract semantic features, and train a random forest classifier to achieve an F-score of 0.74 in binary classification of controls versus people with AD using a set of only 12 features. This is comparable to results (0.72 F-score) with a set of 85 manual features. Adding semantic information to a set of standard lexicosyntactic and acoustic features improves F-score to 0.80. While control and dementia subjects discuss the same topics in the same contexts, controls are more informative per second of speech.\",\n",
       " \"[TITLE] Cross-narrative Temporal Ordering of Medical Events [ABSTRACT] Cross-narrative temporal ordering of medical events is essential to the task of generating a comprehensive timeline over a patient's history. We address the problem of aligning multiple medical event sequences, corresponding to different clinical narratives, comparing the following approaches: (1) A novel weighted finite state transducer representation of medical event sequences that enables composition and search for decoding, and (2) Dynamic programming with iterative pairwise alignment of multiple sequences using global and local alignment algorithms. The cross-narrative coreference and temporal relation weights used in both these approaches are learned from a corpus of clinical narratives. We present results using both approaches and observe that the finite state transducer approach performs performs significantly better than the dynamic programming one by 6.8% for the problem of multiple-sequence alignment.\",\n",
       " '[TITLE] How to Train good Word Embeddings for Biomedical NLP [ABSTRACT] The quality of word embeddings depends on the input corpora, model architectures, and hyper-parameter settings. Using the state-of-the-art neural embedding tool word2vec and both intrinsic and extrinsic evaluations, we present a comprehensive study of how the quality of embeddings changes according to these features. Apart from identifying the most influential hyper-parameters, we also observe one that creates contradictory results between intrinsic and extrinsic evaluations. Furthermore, we find that bigger corpora do not necessarily produce better biomedical domain word embeddings. We make our evaluation tools and resources as well as the created state-of-the-art word embeddings available under open licenses from https://github.com/ cambridgeltl/BioNLP-2016.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = BM25Retriever(docs, referrals)\n",
    "retriever.retrieve('paper that the SWAG paper took hyperparameters from', num_docs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
